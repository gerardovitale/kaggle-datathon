{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from resources.models import random_forest, xgboost, loaded_model\n",
    "from resources.functions.cross_feature import cross_feature\n",
    "from resources.functions.scale_data import normalize_df, standardize_df\n",
    "from resources.functions.get_outliers import get_outlier_per_feature, get_outlier_index_list\n",
    "from resources.functions.model_functions import get_model_predictions, get_stacked_model\n",
    "from resources.properties import PATH_TRAIN, PATH_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Pipeline:\n",
    "1. Drop rows that contain null values,\n",
    "2. Separate numeric and categorical features\n",
    "3. Scale and/or perform feature crossing on numeric features,\n",
    "4. Get outliers index list from numeric features,\n",
    "5. Apply get_dummies to categorical features,\n",
    "6. Concatenate numeric and categorical features\n",
    "7. Remove outliers using Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_train:pd.DataFrame, X_test:pd.DataFrame, file_name_to_save:str, drop_null=True, \n",
    "                 normal=False, stand=False, feature_crossing=False, drop_outliers=True, get_dummies=True):\n",
    "    # 1. droping null values:\n",
    "    if drop_null == True:\n",
    "        # for trainig values\n",
    "        df_train = df_train.dropna(axis=0, how='any')\n",
    "        df_train = df_train.reset_index()\n",
    "        df_train = df_train.drop(columns='ID')\n",
    "        # for testing values\n",
    "        X_test = X_test.reset_index()\n",
    "        X_test = X_test.drop(columns='ID')\n",
    "\n",
    "    # 2. Separating numeric and categorical features:\n",
    "    y_train = df_train.Target\n",
    "    X_train = df_train.drop(axis=1, columns='Target')\n",
    "    # for training values\n",
    "    X_train_numer = X_train.select_dtypes(include=[np.number])\n",
    "    X_train_categ = X_train.select_dtypes(exclude=[np.number])\n",
    "    # for testing values\n",
    "    X_test_numer = X_test.select_dtypes(include=[np.number])\n",
    "    X_test_categ = X_test.select_dtypes(exclude=[np.number])\n",
    "\n",
    "    # 3. Normalizing, Standardize and/or cross feature:\n",
    "    if normal == True:    \n",
    "        # for trainig values\n",
    "        X_train_numer = normalize_df(X_train_numer)\n",
    "        # for testing values\n",
    "        X_test_numer = normalize_df(X_test_numer)\n",
    "    if stand == True:\n",
    "        X_train_numer = standardize_df(X_train_numer)\n",
    "        # for testing values\n",
    "        X_test_numer = standardize_df(X_test_numer)\n",
    "    if feature_crossing == True:\n",
    "        X_train_numer = cross_feature(X_train_numer)\n",
    "        # for testing values\n",
    "        X_test_numer = cross_feature(X_test_numer)\n",
    "        \n",
    "    # 4. Outliers index list:\n",
    "        # for trainig values\n",
    "    outlier_index_list_train = get_outlier_index_list(get_outlier_per_feature, X_train_numer)\n",
    "    \n",
    "    # 5. Dummies:\n",
    "    if get_dummies == True:\n",
    "        # for trainig values\n",
    "        X_train_categ = pd.get_dummies(X_train_categ)\n",
    "        # for testing values\n",
    "        X_test_categ = pd.get_dummies(X_test_categ)\n",
    "\n",
    "    # 6. Concatenation:\n",
    "        # for trainig values\n",
    "    X_train = pd.concat([X_train_categ, X_train_numer], axis=1)\n",
    "    df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        # for testing values\n",
    "    X_test = pd.concat([X_test_categ, X_test_numer], axis=1)\n",
    "   \n",
    "    # 7. Removing outliers:\n",
    "    if drop_outliers == True:\n",
    "        # for trainig values\n",
    "        df_train = df_train.drop(index=outlier_index_list_train)\n",
    "        \n",
    "    # 8. Adjusting X_test:\n",
    "    index = X_train.columns.get_loc(\"PanelG_D\")\n",
    "    values = np.zeros(shape=(X_test.shape[0],), dtype=int)\n",
    "    X_test.insert(loc=index, column='PanelG_D', value=values)\n",
    "    \n",
    "    # 9. Saving data\n",
    "    df_train.to_csv(f'cleaned-data/{file_name_to_save}.csv')\n",
    "    \n",
    "    return {\n",
    "        'DataFrame': df_train,\n",
    "        'file_name': f'cleaned-data/{file_name_to_save}.csv',\n",
    "        'X_test': X_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainig values\n",
    "df_train = pd.read_csv(PATH_TRAIN, index_col=0)\n",
    "\n",
    "# for testing values\n",
    "X_test = pd.read_csv(PATH_TEST, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce the name of the new file prepared\n",
    "file_name_to_save = 'dum-zscore-cross-stan'\n",
    "prepared_data = prepare_data(df_train, X_test, file_name_to_save, drop_null=True, normal=False, \n",
    "                             stand=True, feature_crossing=True, drop_outliers=True, get_dummies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trainig values\n",
    "path_train = f'cleaned-data/{file_name_to_save}.csv'\n",
    "df_train = pd.read_csv(path_train, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models:\n",
    "After study some models performance on the train, it was decided to use the followings:\n",
    "- XGboost\n",
    "- Random Forest\n",
    "- Random Forest Optimized by grid_search\n",
    "- Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models chosen to run get_model_predictions, they were already initialized on models.py\n",
    "models = {\n",
    "    'XGboost': xgboost,\n",
    "    'RandomForest': random_forest,\n",
    "    'RandomForestOptimized': loaded_model,\n",
    "    'Stacking': get_stacked_model(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:25:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# taking the output, df_train (X_train and y_train) and X_test, from prepare_data function runned above\n",
    "y_train = prepared_data['DataFrame'].Target\n",
    "X_train = prepared_data['DataFrame'].drop(axis=1, columns='Target')\n",
    "X_test = prepared_data['X_test']\n",
    "\n",
    "predictions = get_model_predictions(models, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['XGboost', 'RandomForest', 'RandomForestOptimized', 'Stacking'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-107a31e741d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m solution = pd.DataFrame(\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DecisionTree'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Select the model that better perform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_X_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_test' is not defined"
     ]
    }
   ],
   "source": [
    "# create solution DataFrame using predictions dict[model_name]['predictions']\n",
    "# raw_X_test would provide the ID needed when the DataFrame is constructed.\n",
    "raw_X_test = pd.read_csv(PATH_TEST, index_col=0)\n",
    "\n",
    "solution = pd.DataFrame(\n",
    "    data=predictions['RandomForestOptimized']['predictions'], # Select the model predictions that better perform based on the model_comparator results\n",
    "    index=raw_X_test.index, \n",
    "    columns=['Target']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the predictions generated by the model selected previously\n",
    "output_name = 'test-1'\n",
    "solution.to_csv(f'outputs/{output_name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
